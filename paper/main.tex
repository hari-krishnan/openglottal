\documentclass[preprint,12pt]{elsarticle}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{siunitx}
\usepackage[capitalise,noabbrev]{cleveref}
\graphicspath{{./}}
% High-resolution figures for publication (Figure 1, etc.)
\pdfimageresolution=300

% ── journal / short-`hand macros ──────────────────────────────────────────────
\newcommand{\girafe}{\textbf{GIRAFE}}
\newcommand{\bagls}{\textbf{BAGLS}}
\newcommand{\yolounet}{YOLO+UNet}
\newcommand{\yolocrop}{YOLO-Crop+UNet}
\newcommand{\gaw}{GAW}

\journal{Computers in Biology and Medicine}

\begin{document}

\begin{frontmatter}

\title{Detection-Gated Glottal Segmentation with Zero-Shot Cross-Dataset
       Transfer and Clinical Feature Extraction}

\author[1]{Harikrishnan Unnikrishnan}

\address[1]{Orchard Robotics, San Francisco, California 94102, USA}

\begin{abstract}

\noindent\textbf{Background:}
Accurate glottal segmentation in high-speed videoendoscopy (HSV) is
essential for extracting kinematic biomarkers of laryngeal function.
However, existing deep learning models often produce spurious artifacts in
non-glottal frames and fail to generalize across different clinical
settings.

\noindent\textbf{Methods:}
We propose a \emph{detection-gated} pipeline that integrates a
YOLOv8-based detector with a U-Net segmenter.
A temporal consistency wrapper ensures robustness by suppressing false
positives during glottal closure and instrument occlusion.
The model was trained on a limited subset of the \girafe{} dataset
(\num{600} frames) and evaluated via zero-shot transfer on the large-scale
\bagls{} dataset.

\noindent\textbf{Results:}
The pipeline achieved state-of-the-art performance on the \girafe{}
benchmark (DSC \num{0.809}) and demonstrated superior generalizability on
\bagls{} (DSC \num{0.854}, in-distribution) without institutional
fine-tuning.
Downstream validation on a \num{65}-subject clinical cohort confirmed that
automated kinematic features (Open Quotient, coefficient of variation)
remained consistent with established clinical benchmarks.
Specifically, the coefficient of variation (CV) of the glottal area was
found to be a significant marker for distinguishing healthy from
pathological vocal function ($p{=}0.006$).

\noindent\textbf{Conclusions:}
The detection-gated architecture provides a lightweight, computationally
efficient solution (\SI{\sim 35}{frames/s}) for real-time clinical use.
By enabling robust zero-shot transfer, this framework facilitates the
standardized, large-scale extraction of clinical biomarkers across diverse
endoscopy platforms.
Code, trained weights, and evaluation scripts are released at
\url{https://github.com/hari-krishnan/openglottal}.
\end{abstract}

\begin{keyword}
Glottal segmentation \sep High-speed videoendoscopy \sep Vocal fold vibration \sep
Deep learning \sep Glottal area waveform \sep Cross-dataset generalisation
\end{keyword}

\begin{highlights}
\item Detection gate zeroes output after $\leq$3 missed frames, removing spurious detections.
\item SOTA on \girafe{} (DSC \num{0.809}) and \bagls{} (DSC \num{0.854}, in-distribution).
\item YOLO-Crop enables zero-shot transfer to \bagls{} (DSC \num{0.659} at $\tau{=}0.02$).
\item Coefficient of variation distinguishes pathology after sex control.
\item Full pipeline: 502-frame video in \SI{\sim 15}{s} (\SI{\sim 35}{frames/s}) on M-series; U-Net alone \SI{\sim 50}{frames/s}.
\end{highlights}

\end{frontmatter}

% ─────────────────────────────────────────────────────────────────────────────
\section{Introduction}
\label{sec:intro}
% ─────────────────────────────────────────────────────────────────────────────

High-speed videoendoscopy (HSV) enables frame-by-frame observation of vocal
fold vibration at several thousand frames per second, making it the gold
standard for objective voice assessment in clinical laryngology
\cite{deliyski2008laryngoscope}.
The central derived quantity is the \emph{Glottal Area Waveform} (\gaw{})---the
per-frame area of the glottal opening as a function of time---from which
kinematic biomarkers such as open quotient, fundamental frequency, and
vibration regularity can be computed \cite{little2007glottal,deliyski2008laryngoscope,lohscheller2008laryngoscope,patel2014invivo,patel2016effects}.

Accurate glottal segmentation is the bottleneck.
Recent advancements in glottal segmentation have pushed in-distribution
metrics on the large-scale \bagls{} dataset to impressive levels, with
specialized architectures such as the S3AR U-Net achieving a DSC of
\num{88.73}\% \cite{montalbo2024}.
However, these results often fail to translate to the more heterogeneous
conditions of clinical practice.
As demonstrated by Andrade-Miranda et al.\ in the release of the \girafe{}
dataset \cite{andrade2025datainbrief}, standard deep learning models---including
the U-Net (DSC \num{0.643}) and SwinUNetV2 (DSC \num{0.621})---were
outperformed by classical morphological inpainting (DSC \num{0.713}).
This performance degradation highlights a critical lack of generalizability
and robustness in current frame-wise models when faced with the diverse
patient pathologies and technical variabilities of independent clinical
cohorts.
Rule-based methods (active contours, level sets, optical flow) struggle with
the wide variability in illumination, endoscope angle, and patient anatomy
\cite{lohscheller2008laryngoscope}.
Nevertheless, two important gaps remain:

\begin{enumerate}
  \item \textbf{Robustness.}  Clinical recordings routinely contain frames in which
  the glottis is not visible (scope insertion, coughing, endoscope motion)
  \cite{deliyski2008laryngoscope}.
  Existing segmentation models are not equipped to detect this
    condition and produce spurious non-zero area predictions, corrupting the
    downstream \gaw{}.

  \item \textbf{Generalisation.}  Published methods are evaluated on a single
    dataset.  Whether the learned representations transfer to images from a
    different institution, camera system, or patient population is unknown.
\end{enumerate}

We address both gaps with a \emph{detection-gated} pipeline that provides a
\emph{hierarchical decision framework}: the detector acts as a \emph{temporal
consistency guard} (formalised in \cref{sec:temporal_detector}), supplying a
semantic constraint that traditional frame-wise segmentors lack.
We evaluate on two independent public datasets (\cref{sec:experiments}) with
patient-level disjoint splits.
Our contributions are:

\begin{itemize}
  \item A \emph{detection gate} (temporal consistency guard): YOLO-based
    glottis detection acts as a finite-state switch---when YOLO fires,
    U-Net prediction within the detected bounding box is reported; when it
    does not, the previous box is held for at most \num{4} consecutive
    frames (\SI{1}{ms} at \SI{4000}{frames/s}) and then the detection is zeroed.
    This hold applies only in \emph{video} (e.g.\ \girafe{}); \bagls{} is a
    frame-level benchmark (no temporal order), so no hold is applied there.
    Only by zeroing after this short hold do we remove spurious detections
    on non-glottis frames (e.g.\ closed glottis, scope motion) without
    post-hoc filtering.

  \item A \emph{crop-zoom variant} (\yolocrop{}): the detected bounding box
    is cropped and resized to the full U-Net canvas, providing higher
    effective pixel resolution at the glottal boundary and improved
    cross-dataset generalisation.

  \item \emph{End-to-end \gaw{} analysis}: the pipeline is applied to all
    \num{65} \girafe{} patients' full recordings and kinematic features are
    extracted; the coefficient of variation significantly distinguishes
    Healthy from Pathological groups even after controlling for sex
    imbalance.
\end{itemize}

% ─────────────────────────────────────────────────────────────────────────────
\section{Related Work}
\label{sec:related}
% ─────────────────────────────────────────────────────────────────────────────

\subsection{Classical glottal segmentation}
Early methods employed active contours and level-set evolution seeded by
manually placed landmarks \cite{lohscheller2008laryngoscope}.
Optical-flow-based trackers and morphological inpainting variants (InP)
remained competitive for years due to the limited size of labelled datasets
\cite{andrade2025datainbrief}.

\subsection{Deep learning}
The publication of the \girafe{} benchmark
\cite{andrade2025datainbrief} enabled a rigorous comparison: their
U-Net~\cite{ronneberger2015unet} (DSC \num{0.643}) and the transformer-based
SwinUNetV2~\cite{cao2022swinunet} (DSC \num{0.621}) were both outperformed
by the classical InP
method (DSC \num{0.713}), which the authors attributed to the small training
set size.
The \bagls{} dataset \cite{gomez2020bagls} provides \num{55750} training
and \num{3500} test frames from multiple endoscope systems without
patient-level diagnoses, making it a natural testbed for generalisation.
The original \bagls{} paper validated the benchmark with a U-Net baseline
achieving IoU $\approx 0.89$ on the test split.
Subsequent work demonstrated that a single latent bottleneck channel suffices
for accurate glottis segmentation \cite{kist2022latent}, while
Fehling et al.\ incorporated temporal context via a convolutional LSTM
encoder--decoder, reaching DSC \num{0.85} on \num{13000} frames from
\num{130} subjects \cite{fehling2020convlstm}.
Most recently, Nobel et al.\ reported an ensemble UNet--BiGRU segmenter with IoU \num{87.46}
on a private dataset of \num{24000} images~\cite{nobel2024ensemble}, but did not evaluate
on public benchmarks such as \bagls{} or \girafe{}, limiting direct comparison;
by contrast, our lightweight detection-gated pipeline establishes new
SOTA DSC \num{0.809} on \girafe{}, achieves DSC \num{0.854} on \bagls{} in-distribution,
and demonstrates zero-shot transfer to \bagls{}.
While the \bagls{} consortium, led by D\"{o}llinger and colleagues
\cite{dollinger2022}, has established rigorous benchmarks and explored
various re-training strategies for glottis segmentation, institutional
generalizability remains a challenge.
Our detection-gated framework builds upon these efforts, utilizing a
dynamic YOLOv8-based ROI method that achieves an IoU of \num{0.782}
without the need for the incremental fine-tuning or knowledge distillation
proposed in their recent work.
Recent advancements in glottal segmentation, such as the S3AR U-Net
proposed by Montalbo~\cite{montalbo2024}, have pushed in-distribution IoU
metrics to \num{79.97}\% using complex attention-gated and squeeze-and-excitation
modules.
While these lightweight architectures excel at static image benchmarks,
they remain susceptible to non-physiological artifacts in continuous
clinical video.
Our work demonstrates that a simpler U-Net, when coupled with a YOLOv8
detection gate, achieves comparable accuracy (\num{78.2}\% IoU) while
providing the temporal stability necessary to derive statistically
significant clinical biomarkers ($p{=}0.006$).
Kist et al.\ packaged three quality-tiered segmentation networks into the
\emph{Glottis Analysis Tools} (GAT) software for clinical
use~\cite{kist2021gat}.
These temporal and recurrent approaches improve consistency across frames but
require substantially more GPU memory and training data than frame-level
models, limiting their applicability on small datasets such as \girafe{}.

\subsection{Foundation models}
The Segment Anything Model (SAM)~\cite{kirillov2023sam} and its medical
variants~\cite{ma2024medsam} have demonstrated strong zero-shot
segmentation across diverse imaging domains when provided with a point or
bounding-box prompt.
However, SAM's ViT-H backbone (\num{636}M parameters) is an order of
magnitude larger than the pipeline proposed here and requires per-frame
prompting, making it impractical for real-time \gaw{} extraction from
thousands of HSV frames.

\subsection{Detect-then-segment}
Two-stage pipelines---region proposal followed by per-region segmentation---are
standard in general object segmentation \cite{he2017maskrcnn} but have not
been systematically applied to glottal HSV.
Closest to our work, \cite{andrade2015glottal} used a bounding-box
initialisation for active-contour tracking, but did not gate the output
on detection confidence.

\subsection{\gaw{} feature analysis}
Kinematic features of the \gaw{} are well-established clinical measures
\cite{patel2016effects,little2007glottal}. Normative benchmarks for kinematic features,
such as Open Quotient and Speed Quotient, have been established using high-speed videoendoscopy
in typical populations \cite{patel2015kinematic}. Similar normative benchmarks exist for pathological
speakers, including nodules \cite{patel2016nodules}, benign/malignant lesions \cite{pmcid11899851},
and functional dysphonia \cite{schraut2025hoarseness}.
However, the extraction of these
features remains a bottleneck for large-scale clinical application. In this study, we extend
these measures to a 65-subject cohort comprising both healthy and pathological speakers, using a
fully automated detection-gated pipeline to identify discriminative biomarkers.


% ─────────────────────────────────────────────────────────────────────────────
\section{Methods}
\label{sec:method}
% ─────────────────────────────────────────────────────────────────────────────

\subsection{Datasets}
\label{sec:datasets}

\paragraph{\girafe{}}
The \girafe{} dataset \cite{andrade2025datainbrief} contains \num{760}
high-speed laryngoscopy frames (\(256\times256\) px) from \num{65} patients
(adults and children, healthy and pathological) with pixel-level glottal masks
annotated by expert clinicians.
Frames are grouped into official training / validation / test splits (\num{600}
/ \num{80} / \num{80} frames; test patients: 57A3, 61, 63, 64).
Splits are strictly at the patient level: the \num{30} training patients,
\num{4} validation patients, and \num{4} test patients are disjoint sets,
ensuring that no patient's anatomy appears in both training and evaluation.
Each patient folder also contains the full AVI recording (median length
\num{502} frames at \SI{4000}{fps}) and a metadata file recording the disorder
status (Healthy, Paresis, Polyps, Diplophonia, Nodules, Paralysis, Cysts,
Carcinoma, Multinodular Goiter, Other, or Unknown).

\paragraph{\bagls{}}
The Benchmark for Automatic Glottis Segmentation (\bagls{}) \cite{gomez2020bagls}
contains \num{55750} training and \num{3500} test frames from multiple
endoscope types and institutions.
Image dimensions vary (\(256\times256\) to \(512\times512\)); each frame is
paired with a binary glottal mask.
No patient-level labels are provided.
Crucially, \bagls{} was not used in any training step---it serves exclusively
as a zero-shot cross-dataset evaluation set.

\subsection{Pre-processing}
\label{sec:preproc}

\paragraph{\girafe{}}
Images are used at their native \(256\times256\) resolution.

\paragraph{\bagls{} letterboxing}
Variable-size \bagls{} frames are letterboxed to \(256\times256\):
the longer side is scaled to \num{256} pixels while maintaining aspect ratio,
and the remaining dimension is zero-padded symmetrically.
The same transformation is applied identically to the GT mask to maintain
spatial correspondence.

\subsection{YOLO Glottis Detector and Temporal Consistency Guard}
\label{sec:yolo}

We fine-tune YOLOv8n \cite{jocher2023yolo} on bounding boxes derived from
the \girafe{} training split.
GT bounding boxes are computed as the tight enclosing rectangle of each GT
mask, then converted to YOLO label format.
Training runs for \num{2} epochs using the default YOLOv8 augmentation
pipeline.

At inference time we apply a \emph{temporal consistency model} that gates
the segmentor output without the memory overhead of 3D convolutions or
recurrent architectures~\cite{fehling2020convlstm}.
The model is defined by a detection process $\{B_t\}$ and a gating rule as
follows.

\paragraph{Formal definition (4-frame, \SI{1}{ms}, hold)}
Let $B_t \in \{0,1\}$ denote that the detector produced a valid glottis
bounding box at frame $t$ ($B_t=1$) or did not ($B_t=0$).
Let $M_t$ denote the raw U-Net segmentation mask at $t$ and
$\mathcal{R}_t$ the bounding box at $t$ (held from the last detection when
$B_t=0$).
The \emph{gated output} $\widehat{M}_t$ is defined by the constraint
\begin{equation}
  \widehat{M}_t =
  \begin{cases}
    M_t \big|_{\mathcal{R}_t} & \text{if } \sum_{i=\max(1,\,t-3)}^{t} B_i > 0, \\
    \mathbf{0} & \text{otherwise},
  \end{cases}
\end{equation}
where $M_t|_{\mathcal{R}_t}$ denotes the restriction of the mask to the
box $\mathcal{R}_t$ (pixels outside $\mathcal{R}_t$ are zero) and
$\mathbf{0}$ is the zero mask.
Thus the segmentor is \emph{deactivated} (output zeroed) if and only if
there has been no detection in the window $\{t-3,\,t-2,\,t-1,\,t\}$ (four
frames, $\approx\,\SI{1}{ms}$ at \SI{4000}{frames/s}); once
$B_{t'}=1$ for some $t'>t$, output is restored.
The detected box centre is drift-clamped to at most \num{30} pixels per
frame to reject spurious jumps; the box \emph{size} is updated on each
fresh detection.
This temporal consistency model removes spurious detections (e.g.\ stale
boxes from closed glottis or scope motion) while preserving the natural
opening--closing motion of the glottis.
It is used when processing \emph{video} (e.g.\ \girafe{}); for frame-level
benchmarks such as \bagls{}, where frames have no temporal order, the
detector is run per frame with no temporal state.
Because all temporal reasoning is confined to this gating layer, the
U-Net remains a standard 2D model that can be trained on the small
\girafe{} training set (\num{600} frames) without risk of temporal
overfitting.
\label{sec:temporal_detector}

\subsection{U-Net Segmenter}
\label{sec:unet}

We train two U-Net~\cite{ronneberger2015unet} variants with a four-level
encoder--decoder (channel widths \(32, 64, 128, 256\), \num{7.76}M
parameters).

\paragraph{Full-frame U-Net (\texttt{unet\_only})}
Input: \(256\times256\) grayscale frame.
Training data: \num{600} \girafe{} training frames with augmentation
(random flips, $\pm\SI{30}{\degree}$ rotation, $\pm\SI{15}{\percent}$ scale jitter, brightness /
contrast / Gaussian blur perturbations).
Loss: \(0.5\cdot\mathcal{L}_{\mathrm{BCE}} + 0.5\cdot\mathcal{L}_{\mathrm{DSC}}\)~\cite{milletari2016vnet}.
Optimiser: AdamW~\cite{loshchilov2019adamw}, learning rate \num{e-3},
cosine annealing~\cite{loshchilov2017sgdr} over \num{50} epochs (with early stopping).

\paragraph{Crop-mode U-Net (\texttt{unet\_crop})}
For each training frame, the YOLO detector is run and the detected bounding
box (plus 8 px padding on each side) is cropped and resized to
\(256\times256\).
The matching GT mask undergoes the same crop-resize.
Frames with no YOLO detection are excluded (\num{487} training crops /
\num{77} validation crops retained out of \num{600} / \num{80} frames).
Training procedure is identical to the full-frame model, saving to a separate
checkpoint.

\subsection{Inference Pipelines}
\label{sec:pipelines}

\begin{figure}[t]
\centering
\IfFileExists{pipeline.png}{%
  \includegraphics[width=\linewidth,keepaspectratio]{pipeline.png}%
}{%
  \IfFileExists{pipeline.pdf}{%
    \includegraphics[width=\linewidth,keepaspectratio]{pipeline.pdf}%
  }{%
    \fbox{\parbox{0.85\linewidth}{\centering\vspace{1.5cm}%
      \textbf{Figure 1:} Overview of the three main inference pipelines.\\
      Run \texttt{npx -y @mermaid-js/mermaid-cli -i fig1\_pipeline.mmd -o pipeline.png -w 1000 -s 2 -b white} in \texttt{paper/}.\\
      Check spelling: ``YOLO detector'' and ``Gate'' (not detectir/Gane).\vspace{1.5cm}}}
  }%
}
\caption{Overview of the three main inference pipelines.
  Input (left) is the grayscale frame; each pipeline
  yields a segmentation mask (right).
  Solid arrows denote data flow; the gate symbol indicates that the output is
  set to zero when the detector does not fire (or after at most
  \num{4} consecutive missed frames), removing spurious detections.}
\label{fig:pipeline}
\end{figure}

Five pipelines are evaluated (\cref{fig:pipeline}):

\paragraph{U-Net only}
Run the full-frame U-Net on the \(256\times256\) grayscale input; output the
thresholded probability map directly.
No detection gate---every frame produces a prediction.

\paragraph{\yolounet{}}
(1) Run the detector on the full frame.
(2) Run the full-frame U-Net on the full frame.
(3) Zero the U-Net mask outside the detected bounding box.
If the detector does not fire (or after \num{4} consecutive misses), output is
all-zero, removing spurious detections.

\paragraph{\yolocrop{}}
(1) Run the detector.
(2) Crop the detected region (\(+8\) px padding), resize to \(256\times256\).
(3) Run the crop-mode U-Net on the resized crop.
(4) Resize the output mask back to the original crop dimensions.
(5) Paste into a full-frame zero mask at the detected coordinates.
If the detector does not fire (or after \num{4} consecutive misses), output is
all-zero.

\paragraph{Motion (baseline)}
A motion-based tracker within the detected region (adapted from~\cite{patel2015kinematic}); first frames used for initialisation, excluded from metrics.

\paragraph{OTSU (baseline)}
Otsu thresholding~\cite{otsu1979threshold} (inverted, glottis is dark) within the detected bounding box; no learned segmentation component.

\subsection{Glottal Area Waveform Features}
\label{sec:gaw_features}

For each patient video the \yolounet{} pipeline is applied to every frame,
yielding an area waveform $A(t)$.
As in the pipeline definition (\cref{sec:pipelines}), the detector acts as a
gate: frames where the detector does not fire (or after at most
\num{4} consecutive missed frames the detection is zeroed) contribute zero
to the waveform, removing spurious detections and avoiding non-zero
area from off-target endoscope views.
Seven scalar kinematic features are extracted (\cref{tab:features}), chosen
for their established clinical utility in distinguishing normal from
disordered voices~\cite{patel2015kinematic}.
The fundamental frequency $f_0$ is estimated from the dominant FFT peak and
converted from cycles/frame to Hz using the recording frame rate.
Features are compared between Healthy (\(n{=}15\)) and Pathological
(\(n{=}25\)) groups using the two-sided Mann--Whitney $U$ test (significance
threshold $\alpha{=}0.05$); the \num{25} patients with Unknown or other disorder status
are excluded from the group comparison.

\begin{table}[t]
\centering
\caption{Kinematic features extracted from the Glottal Area Waveform.}
\label{tab:features}
\begin{tabular}{ll}
\toprule
Feature & Description \\
\midrule
\texttt{area\_mean}    & Mean glottal area (px$^2$) over open frames \\
\texttt{area\_std}     & Standard deviation of area \\
\texttt{area\_range}   & Max $-$ min area (vibratory excursion) \\
\texttt{open\_quotient}& Fraction of cycle with area $>$ 10\% of mean \\
\texttt{f0}            & Dominant frequency from FFT (Hz) \\
\texttt{periodicity}   & Peak autocorrelation at lags 1--50 \\
\texttt{cv}            & Coefficient of variation (\texttt{area\_std} / \texttt{area\_mean}) \\
\bottomrule
\end{tabular}
\end{table}

% ─────────────────────────────────────────────────────────────────────────────
\section{Experiments}
\label{sec:experiments}
% ─────────────────────────────────────────────────────────────────────────────

\subsection{Evaluation Metrics}
\label{sec:metrics}

\begin{itemize}
  \item \textbf{Det.Recall}: fraction of frames where the YOLO detector fired
    (relevant for YOLO-gated pipelines; reported as \num{1.000} for detection-free
    baselines that always output a prediction).
  \item \textbf{DSC}: \(2\mathrm{TP}/(2\mathrm{TP}+\mathrm{FP}+\mathrm{FN})\),
    computed per frame then averaged.
  \item \textbf{IoU}: \(\mathrm{TP}/(\mathrm{TP}+\mathrm{FP}+\mathrm{FN})\),
    per frame then averaged.
  \item \textbf{DSC${\geq}0.5$}: fraction of frames with DSC $\geq 0.5$,
    a clinical pass/fail threshold \cite{andrade2025datainbrief}.
\end{itemize}

\subsection{Implementation Details}
All experiments run on Apple M-series hardware (MPS backend).
YOLO training: YOLOv8n, \num{2} epochs, default hyperparameters, image size
\(256\times256\).
U-Net training: \num{50} epochs (with early stopping), batch size \num{16}, AdamW (\texttt{lr=1e-3}),
cosine annealing.
Both models trained solely on \girafe{} training split.

% ─────────────────────────────────────────────────────────────────────────────
\section{Results}
\label{sec:results}
% ─────────────────────────────────────────────────────────────────────────────

\subsection{GIRAFE In-Distribution Evaluation}
\label{sec:results_girafe}

\Cref{tab:girafe} compares our pipelines against the published \girafe{}
baselines on the \num{80}-frame test split.
Our U-Net alone achieves the highest DSC (\num{0.809}) and clinical pass
rate (DSC${\geq}0.5 = \SI{96.2}{\%}$), substantially outperforming all
three published methods.
The detection-gated \yolounet{} pipeline reaches DSC \num{0.746},
still surpassing InP (\num{0.713}) and SwinUNetV2 (\num{0.621}).
The gap between U-Net only and \yolounet{} on \girafe{} arises because the
detected bounding box occasionally clips GT glottis pixels that extend beyond
the detected region; this cost is absent without gating.
The detector fires on \SI{95}{\%} of test frames (Det.Recall $= 0.95$);
the remaining \SI{5}{\%} are zeroed after the 4-frame (\SI{1}{ms} at \SI{4000}{frames/s}) hold, consistent with
occasional closed-glottis or low-confidence frames.
However, the detection gate provides essential robustness on real clinical
recordings where the endoscope may be off-target (\cref{sec:results_gaw}).

The \yolocrop{} pipeline, trained on YOLO-cropped patches, achieves DSC
\num{0.697}---below \yolounet{} but above both deep-learning baselines from
the \girafe{} paper.
The performance gap relative to \yolounet{} stems from the \girafe{} test
frame structure: the \num{80} test frames are the \emph{first} \num{20} frames
per patient, and the tight detected bounding box occasionally clips GT glottis
pixels that extend marginally beyond the detected region.
Crucially, this limitation is overcome in the cross-dataset setting where the
glottis region is larger relative to the frame (\cref{sec:results_bagls}).

To evaluate the necessity of a deep segmentation head, we compared the
proposed pipeline against two non-learned baselines: a motion-based
tracking method (Motion) adapted from~\cite{patel2015kinematic}, and
Otsu thresholding~\cite{otsu1979threshold} within the detected region (OTSU).
As shown in \cref{tab:girafe}, the motion-based approach struggled with
noise and motion artifacts, yielding a DSC of \num{0.265}; the OTSU baseline
fared worse (DSC \num{0.224}) under variable illumination and contrast.
Both comparisons justify the use of the U-Net segmenter.

\begin{table}[t]
\centering
\caption{Segmentation results on the \girafe{} test split (4 patients,
  80 frames). Published baselines from \protect\cite{andrade2025datainbrief}.
  Det.Recall $=$ n/a for methods that do not include a detection stage.}
\label{tab:girafe}
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccc}
\toprule
Method & Det.Recall & DSC & IoU & DSC${\geq}0.5$ \\
\midrule
InP~\cite{andrade2025datainbrief}       & n/a   & 0.713 & n/a   & n/a \\
U-Net~\cite{andrade2025datainbrief}     & n/a   & 0.643 & n/a   & n/a \\
SwinUNetV2~\cite{andrade2025datainbrief}& n/a   & 0.621 & n/a   & n/a \\
\midrule
U-Net only (ours)                       & n/a   & \textbf{0.809} & \textbf{0.699} & \textbf{96.2\%} \\
\yolounet{} (ours)                      & 0.950 & 0.746 & 0.642 & 88.8\% \\
\yolocrop{} (ours)                      & 0.950 & 0.697 & 0.567 & 77.5\% \\
\midrule
OTSU (baseline)                         & 0.950 & 0.224 & 0.133 &  2.5\% \\
Motion (baseline)                       & 0.950 & 0.265 & 0.165 &  9.7\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Ablation: hold duration}
We varied the number of frames the detector holds the last bounding box when
YOLO misses (0--20 and $\infty$) on the \girafe{} test set (\cref{fig:hold_ablation}).
As illustrated in \cref{fig:hold_ablation}, the segmentation performance
(DSC) and detection success rate exhibit a sharp increase as the temporal
hold duration rises from 0 to 4 frames.
Beyond this \SI{1}{ms} threshold, the metrics plateau, suggesting that the
temporal gate has successfully bridged the physiological closed-phase of
the glottal cycle.
The slight decline in DSC at higher hold values justifies our selection of
a 4-frame window as the optimal balance between artifact suppression and
temporal sensitivity.
While the optimal hold duration is coupled to the video frame rate, this
ablation study demonstrates that a temporal window of approximately
\SI{1}{ms} effectively suppresses transient segmentation artifacts without
compromising the capture of high-frequency glottal dynamics.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth,keepaspectratio]{hold_ablation.pdf}
\caption{Effect of temporal hold duration (0--20 frames and $\infty$) on
  \yolounet{} (GIRAFE test set): DSC (left axis) and Det.Recall (right axis).
  At \SI{4000}{frames/s}, \num{4} frames $=$ \SI{1}{ms}.}
\label{fig:hold_ablation}
\end{figure}

\Cref{fig:montage} shows an example of the pipeline output: a montage of
\num{12} annotated frames from patient~1 over one vibratory cycle, with the
glottal opening segmented (green) and the detected region boxed (yellow); the
numeric label in each frame is the glottal area in pixels².

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth,keepaspectratio]{patient1_montage.png}
\caption{Output of the \yolounet{} pipeline on \num{12} evenly spaced frames
  from one patient (patient~1): glottal mask (green), YOLO bounding box
  (yellow), and per-frame area. The montage illustrates temporal consistency
  of the segmentation across the vibratory cycle.}
\label{fig:montage}
\end{figure}

\subsection{Zero-Shot Cross-Dataset Evaluation on BAGLS}
\label{sec:results_bagls}

\bagls{} is a frame-level benchmark (images are not ordered as video).
The temporal hold is therefore not applied; the detector is run independently
per frame.
\Cref{tab:bagls} reports results on \num{3500} \bagls{} test frames.
Neither the U-Net nor the YOLO weights were trained on any \bagls{} data.
This zero-shot setting is the \emph{state-of-the-art comparison for
generalisation}: while other models can achieve higher DSC on \bagls{} when
trained on \bagls{}, ours is the most generalisable without retraining,
demonstrating that YOLO-based cropping normalises the input space across
camera distance and institutional differences.

The YOLO detector fires on \SI{68.8}{\%} of \bagls{} frames
(Det.Recall $= 0.688$), confirming a degree of domain shift from the
\girafe{}-trained detector.
On frames where YOLO does not detect, the output is correctly zeroed.

At the default threshold ($\tau{=}0.25$), \yolocrop{} achieves the
highest DSC (\num{0.609}) and DSC${\geq}0.5$ (\SI{70.3}{\%}),
outperforming unguided U-Net inference (\num{0.588} DSC,
\SI{67.1}{\%} DSC${\geq}0.5$).
The plain \yolounet{} pipeline scores \num{0.545}---below unguided U-Net---because
the \SI{31.2}{\%} of frames with no YOLO detection receive a zero mask that
lowers the mean; when YOLO does detect, restricting the full-frame U-Net
output to the bounding box clips some GT pixels.
\yolocrop{} avoids this by rescaling the detected region, giving U-Net
higher effective resolution at the glottis boundary and compensating for the
missed frames through better per-frame accuracy.
Lowering the confidence threshold to $\tau{=}0.02$ further improves
\yolocrop{} to DSC \num{0.659} (\cref{tab:bagls_sweep}).
While specialized architectures trained directly on \bagls{} can reach DSC
values exceeding \num{0.88} \cite{montalbo2024}, our pipeline achieves a
zero-shot DSC of \num{0.609} with no institutional fine-tuning.
This represents a robust baseline for `plug-and-play' clinical deployment
where labeled data from a specific endoscopic system may not be available.

\begin{table}[t]
\centering
\caption{Zero-shot cross-dataset results on \bagls{} test set
  (\num{3500} frames). No \bagls{} data used in training.
  Det.Recall shown as 1.000 for U-Net~only (no YOLO gate; always processes
  every frame).}
\label{tab:bagls}
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccc}
\toprule
Method & Det.Recall & DSC & IoU & DSC${\geq}0.5$ \\
\midrule
U-Net only                     & 1.000 & 0.588 & 0.504 & 67.1\% \\
\yolounet{} (ours)             & 0.688 & 0.545 & 0.473 & 61.9\% \\
\yolocrop{} (ours)             & 0.688 & 0.609 & 0.533 & 70.3\% \\
\bottomrule
\end{tabular}
\end{table}

When U-Net and YOLO are trained on \bagls{} (in-distribution evaluation on the
same \num{3500} test frames), performance is substantially higher
(\cref{tab:bagls_indist}).
U-Net only reaches DSC \num{0.846} and DSC${\geq}0.5 = \SI{94.0}{\%}$;
\yolounet{} achieves the best segmentation (DSC \num{0.854}, IoU \num{0.782},
\SI{94.6}{\%} DSC${\geq}0.5$) with detection recall \num{0.865};
\yolocrop{} reaches DSC \num{0.742} and \SI{87.1}{\%} DSC${\geq}0.5$.
The YOLO detector attains precision \num{0.982} and recall \num{0.974}
(TP\,=\,\num{2972}, FP\,=\,\num{54}, FN\,=\,\num{80}), indicating that
BAGLS-trained weights transfer well to the held-out test split.
Our \num{0.854} DSC surpasses the benchmark U-Net baseline~\cite{gomez2020bagls} and reported diffusion-refined segmentation (DSC \num{0.80})~\cite{wu2024medsegdiff}.
While Döllinger et al.\ \cite{dollinger2022} report a mean IoU of \num{0.77}
on \bagls{} using a semi-automated Region of Interest (ROI) method, our
detection-gated pipeline achieves a superior IoU of \num{0.782} (see
\cref{tab:bagls_indist}) through dynamic YOLOv8-based cropping.
Furthermore, unlike prior efforts that require complex incremental
fine-tuning or knowledge distillation to adapt to new recording modalities,
our architecture maintains high clinical utility ($p{=}0.006$) through a
robust zero-shot transfer framework, eliminating the need for institutional
re-training.

\begin{table}[t]
\centering
\caption{In-distribution results on \bagls{} test set (\num{3500} frames)
  with BAGLS-trained U-Net and YOLO weights.
  Det.Recall shown as 1.000 for U-Net only (no YOLO gate).}
\label{tab:bagls_indist}
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccc}
\toprule
Method & Det.Recall & DSC & IoU & DSC${\geq}0.5$ \\
\midrule
U-Net only                     & 1.000 & 0.846 & 0.772 & 94.0\% \\
\yolounet{} (ours)             & 0.865 & 0.854 & 0.782 & 94.6\% \\
\yolocrop{} (ours)             & 0.865 & 0.742 & 0.637 & 87.1\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Confidence threshold sensitivity}
The default YOLO confidence threshold ($\tau{=}0.25$) was inherited from the
\girafe{} in-distribution setting.
Because the \girafe{}-trained detector exhibits domain shift on \bagls{},
many true glottis frames receive detection scores below \num{0.25} and are
incorrectly suppressed.
\Cref{tab:bagls_sweep} reports a single-pass threshold sweep: YOLO inference
is run once at $\tau{=}0.001$ and the confidence scores are thresholded in
post-processing.
Lowering $\tau$ to \num{0.02} raises \yolocrop{} detection recall from
\SI{68.8}{\%} to \SI{85.9}{\%} and DSC from \num{0.609} to \num{0.659}
($+0.050$), with the clinical pass rate increasing from \SI{70.3}{\%} to
\SI{76.3}{\%}.
Below $\tau{=}0.02$ performance plateaus and then degrades as false-positive
detections introduce noisy bounding boxes.

\begin{table}[t]
\centering
\caption{Effect of YOLO confidence threshold on \yolocrop{} performance
  (\bagls{} test, \num{3500} frames, zero-shot).
  YOLO inference is run once; thresholds are applied in post-processing.}
\label{tab:bagls_sweep}
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{rcccc}
\toprule
$\tau$ & Det.Recall & DSC & IoU & DSC${\geq}0.5$ \\
\midrule
0.001 & 0.943 & 0.646 & 0.553 & 75.0\% \\
0.005 & 0.917 & 0.652 & 0.561 & 75.7\% \\
0.01  & 0.895 & 0.654 & 0.563 & 75.8\% \\
\textbf{0.02}  & \textbf{0.859} & \textbf{0.659} & \textbf{0.568} & \textbf{76.3\%} \\
0.03  & 0.842 & 0.656 & 0.567 & 76.0\% \\
0.05  & 0.819 & 0.652 & 0.565 & 75.6\% \\
0.10  & 0.773 & 0.641 & 0.558 & 74.3\% \\
0.25  & 0.688 & 0.609 & 0.533 & 70.3\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Technical Validation: Glottal Area Waveform Features}
\label{sec:results_gaw}

The kinematic features extracted in this study---including Open Quotient
(OQ), coefficient of variation (cv), and related measures (\cref{tab:features})---were
selected based on their established clinical utility in differentiating
vocal pathologies, as demonstrated by Patel et al.~\cite{patel2015kinematic}.
While the diagnostic value of these parameters is well-documented, their
widespread clinical adoption has been limited by the need for robust,
automated segmentation.
Our detection-gated pipeline addresses this gap by providing a
zero-shot-capable framework that extracts these features with high
temporal consistency across institutional datasets (\girafe{} and \bagls{}).
Accuracy is benchmarked on \girafe{} and \bagls{} (DSC\slash IoU); the
\num{65}-subject \girafe{} cohort serves as the primary benchmark for
\emph{clinical reproducibility}---i.e.\ whether the automated pipeline
replicates group differences previously established with manual or
semi-automated methods~\cite{patel2015kinematic,patel2016effects}.

To validate that the pipeline produces clinically meaningful output, we
extract kinematic \gaw{} features from all \num{65} \girafe{} patient
recordings and test whether the automatically derived features replicate
known group differences between Healthy and Pathological voices.
The clinical goal is not merely to maximise DSC but to preserve
\emph{downstream discriminants} such as the coefficient of variation (cv)
of the glottal area, which reflects vibratory regularity.
\Cref{tab:gaw} reports seven features for the \num{15} Healthy and
\num{25} Pathological patients (\num{25} patients with Unknown or Other status are
excluded).
This analysis is exploratory---given the small sample sizes and multiple
features tested, we report uncorrected $p$-values from two-sided
Mann--Whitney $U$ tests ($\alpha = 0.05$) without multiple-comparison
correction.

The Healthy and Pathological groups are sex-imbalanced: Healthy
recordings are \num{80}\% female (\num{12}F/\num{3}M) while Pathological
recordings are \num{56}\% male (\num{14}M/\num{11}F; Fisher's exact
$p{=}0.025$).
Because $f_0$ is strongly sex-dependent (males \SI{100.3}{Hz} vs.\
females \SI{223.5}{Hz}, $p{<}0.001$), we report results stratified by
sex (\cref{tab:gaw}) rather than pooled.

In the female subgroup (\num{12} Healthy vs.\ \num{11} Pathological),
$f_0$ does not reach significance ($p{=}0.156$), indicating that any
apparent difference in the unstratified data is driven by sex composition.
In contrast, cv is the only feature that distinguishes groups after
stratification:

\begin{itemize}
  \item \textbf{Coefficient of variation (cv, female only)}:
    \(0.95\pm0.20\) (Healthy) vs.\ \(0.57\pm0.29\) (Pathological),
    $p{=}0.006$.
\end{itemize}

\noindent Healthy voices exhibit significantly higher vibration
variability---consistent with the established observation that laryngeal
pathologies increase vocal fold mass and stiffness, reducing the amplitude
of glottal oscillation
\cite{patel2016effects,little2007glottal}.
This automated finding aligns with the variability trends reported in the
JSLHR cohort~\cite{patel2015kinematic}: the pipeline effectively
``sees'' what clinicians see when distinguishing Healthy from Pathological
voices.
In the male subgroup (\num{3} Healthy vs.\ \num{14} Pathological), cv
shows the same directional trend ($0.75$ vs.\ $0.63$) but does not reach
significance ($p{=}0.509$), as expected given the very small Healthy
sample.  Periodicity approaches significance in males ($p{=}0.068$),
suggesting it may also distinguish groups with a larger cohort.

\begin{table}[t]
\centering
\caption{Glottal area waveform kinematic features: Healthy (H) vs.\
  Pathological (P), stratified by sex.
  These results constitute an \emph{automated replication} of the
  manual\slash semi-automated benchmarks established by Patel et al.\ in
  JSLHR~\cite{patel2015kinematic}; the pipeline preserves the coefficient
  of variation (cv), the key clinical discriminant (bold).
  $p$-values from two-sided Mann--Whitney $U$; bold = $p{<}0.05$.
  The male subgroup has only $n{=}3$ Healthy recordings and results
  should be interpreted with caution.}
\label{tab:gaw}
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l ccc ccc}
\toprule
 & \multicolumn{3}{c}{Female (12\,H / 11\,P)}
 & \multicolumn{3}{c}{Male (3\,H / 14\,P)} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
Feature & H & P & $p$ & H & P & $p$ \\
\midrule
area\_mean
  & $125.2{\pm}43.1$  & $247.8{\pm}204.6$ & 0.230
  & $192.1{\pm}18.3$  & $172.7{\pm}94.0$  & 0.768 \\
area\_std
  & $112.9{\pm}32.2$  & $118.9{\pm}96.0$  & 0.406
  & $142.7{\pm}35.0$  & $92.0{\pm}66.9$   & 0.197 \\
area\_range
  & $336.7{\pm}97.6$  & $375.5{\pm}272.2$ & 0.559
  & $439.7{\pm}86.7$  & $343.1{\pm}212.3$ & 0.488 \\
open\_quot.
  & $0.760{\pm}0.207$ & $0.874{\pm}0.131$ & 0.192
  & $0.860{\pm}0.145$ & $0.843{\pm}0.186$ & 1.000 \\
$f_0$ (Hz)
  & $241.7{\pm}34.8$  & $203.5{\pm}73.6$  & 0.156
  & $183.3{\pm}75.0$  & $82.5{\pm}79.3$   & 0.169 \\
periodicity
  & $0.955{\pm}0.008$ & $0.946{\pm}0.013$ & 0.255
  & $0.962{\pm}0.001$ & $0.900{\pm}0.116$ & 0.068 \\
\textbf{cv}
  & $\mathbf{0.95{\pm}0.20}$   & $\mathbf{0.57{\pm}0.29}$   & \textbf{0.006}
  & $0.75{\pm}0.19$   & $0.63{\pm}0.40$   & 0.509 \\
\bottomrule
\end{tabular}
\end{table}

% ─────────────────────────────────────────────────────────────────────────────
\section{Discussion}
\label{sec:discussion}
% ─────────────────────────────────────────────────────────────────────────────

\paragraph{Detection gating as a clinical safety mechanism}
The detection gate provides a qualitative benefit that segmentation metrics
alone do not capture: after at most \num{3} consecutive frames without a
YOLO detection the output is zeroed, so the \gaw{} is zero-valued when the
endoscope has moved away from the glottis (or the glottis is closed), rather
than containing artefactual non-zero area from spurious U-Net activations.
This matters in practice because a clinician computing open quotient or
periodicity over a full recording would otherwise need to manually identify
and excise off-target frames---a laborious and subjective step.

\paragraph{Why \yolocrop{} generalises better}
On the in-distribution \girafe{} test, \yolounet{} outperforms \yolocrop{}.
On \bagls{}, the order reverses.
We attribute this to two factors.
First, \bagls{} images span a wider range of glottis sizes and aspect ratios
(frames from \(256\times120\) to \(512\times512\) pixels, with the glottis
occupying a variable fraction of the image).
By normalising the glottis to fill the U-Net canvas, \yolocrop{} removes
this scale variability and presents a consistent input distribution to the
model.
Second, the \bagls{} glottis appears at relatively lower resolution in the
full letterboxed frame than in \girafe{}; the crop step recovers this
resolution.

\paragraph{Experimental direction and data efficiency}
A natural alternative would be training on the larger \bagls{} dataset
(\num{55750} frames) and performing zero-shot transfer to \girafe{}.
However, we intentionally prioritised the inverse direction for two reasons.
First, the clinical objective of this work---technical validation of \gaw{}
biomarkers---requires the highest possible segmentation accuracy on the
patient-labelled \girafe{} recordings.
Second, demonstrating that a model trained on only \num{600} frames can
generalise ``upwards'' to the heterogeneous, multi-institutional \bagls{}
dataset provides a more rigorous test of the pipeline's robustness.
This approach proves that the \yolocrop{} mechanism effectively learns
glottal anatomy rather than merely memorising institutional imaging
characteristics.

\paragraph{Why our U-Net outperforms the \girafe{} U-Net baseline}
Our U-Net alone achieves DSC \num{0.809}, significantly beating the
original \girafe{} benchmark U-Net (DSC \num{0.643})~\cite{andrade2025datainbrief}, despite using the
same dataset, split, and a comparable augmentation pipeline (rotation,
scaling, flipping, Gaussian noise/blur, brightness/contrast).
Three training-recipe differences account for the gap:
\emph{(i)~Grayscale input} (1 channel vs.\ 3-channel RGB)---the glottal gap
is defined by intensity contrast, so colour triples the input dimensionality
without adding discriminative signal, making the network harder to train on
only \num{600} frames;
\emph{(ii)~Combined BCE\,+\,Dice loss} versus Dice loss only---the BCE term
supplies stable per-pixel gradients that complement the region-level DSC
objective and avoid the gradient instability of pure DSC near $0$ or $1$;
\emph{(iii)~Higher learning rate with cosine annealing}
($10^{-3}$ vs.\ fixed $2{\times}10^{-4}$) and
AdamW~\cite{loshchilov2019adamw} instead of Adam~\cite{kingma2015adam},
which together explore the loss landscape more aggressively and converge in
\num{50} epochs to a stronger minimum than \num{200} epochs at a fixed low
rate.
These are straightforward engineering choices rather than architectural
novelties, yet they yield a $+0.166$ DSC improvement---underscoring that on
small medical-imaging datasets the training recipe matters as much as model
design.

\paragraph{Lightweight pipeline vs.\ foundation models}
Foundation models such as SAM~\cite{kirillov2023sam} and
MedSAM~\cite{ma2024medsam} offer impressive zero-shot segmentation but
require a per-frame bounding-box or point prompt---precisely what our YOLO
detector already provides.
Using YOLO as the SAM prompter is conceptually possible; however, SAM's
ViT-H encoder (\num{636}M parameters, ${\sim}$\SI{150}{ms} per frame on
GPU) is over $80\times$ larger than our U-Net (\num{7.76}M parameters);
combined with YOLOv8n (\num{3.2}M parameters), our full pipeline totals
${\sim}$\num{11}M parameters versus ${\sim}$\num{636}M for SAM, justifying
the lightweight design for clinical hardware.
SAM would make real-time \gaw{} extraction from clinical recordings
($>$\num{1000} frames at $>$\SI{1000}{fps} capture rate) impractical
without dedicated hardware.
On Apple M-series hardware (MPS backend), U-Net alone reaches
${\sim}$\SI{50}{frames\per\second} (a \num{502}-frame video in
${\sim}$\SI{10}{s}); the full detection-gated pipeline (YOLO + U-Net)
processes the same video in approximately \SI{15}{s}
(${\sim}$\SI{35}{frames\per\second}), well within offline clinical workflow
requirements.
Exploring SAM-based distillation to further improve U-Net accuracy without
sacrificing throughput is an interesting direction for future work.

\paragraph{Benchmarks vs.\ private datasets}
While complex ensembles such as UNet--BiGRU achieve high accuracy on private
laryngeal datasets~\cite{nobel2024ensemble}, their lack of evaluation on public
benchmarks limits reproducibility and comparability.
Our \num{7.76}M-parameter detection-gated pipeline instead establishes
state-of-the-art performance on \girafe{} (DSC \num{0.809}) and
in-distribution \bagls{} (DSC \num{0.854}), while also demonstrating
zero-shot cross-dataset generalisation---a combination of openness and
robustness that is absent from prior work.
Whereas recent work such as the S3AR U-Net~\cite{montalbo2024} relies on
complex attention-gated and squeeze-and-excitation modules for static image
benchmarks, our \emph{detection-gated} architecture (\cref{fig:pipeline})
provides superior temporal stability in continuous clinical video by zeroing
output when no glottis is detected, avoiding non-physiological artifacts.
Similarly, whereas the \bagls{} consortium~\cite{dollinger2022} addressed
generalisation via incremental re-training and knowledge distillation, our
\yolocrop{} variant achieves zero-shot transfer to \bagls{} (DSC \num{0.659}
at $\tau{=}0.02$) without institutional re-training, reducing the deployment
burden for new recording modalities.

\paragraph{YOLO detection and confidence tuning}
At the default threshold ($\tau{=}0.25$) the YOLO detector fires on
only \SI{68.8}{\%} of \bagls{} frames.
Lowering $\tau$ to \num{0.02} recovers \SI{85.9}{\%} recall and lifts
\yolocrop{} DSC from \num{0.609} to \num{0.659}
(\cref{tab:bagls_sweep}).
Below $\tau{=}0.02$, false-positive detections introduce noisy bounding
boxes that degrade the crop, so performance peaks at this threshold.
Fine-tuning the detector on a small \bagls{} subset would likely raise
recall further and is left as future work.

\paragraph{Technical validation of \gaw{} features}
The \gaw{} analysis is not intended as a clinical study of new biomarkers;
rather, it serves as a technical validation that the fully automated
pipeline replicates the group differences (Healthy vs.\ Pathological)
established through manual or semi-automated analysis in the JSLHR
literature~\cite{patel2015kinematic,patel2016effects,little2007glottal}.
The key result is that the coefficient of variation (cv) significantly
distinguishes Healthy from Pathological voices ($p{=}0.006$, female
subgroup)---demonstrating that the pipeline is not merely accurate at the
pixel level (DSC\slash IoU) but yields \emph{clinically useful} biomarkers.
\girafe{} and \bagls{} are the primary benchmarks for segmentation
accuracy (DSC\slash IoU); the \num{65}-subject \girafe{} cohort is the
primary benchmark for \emph{clinical reproducibility} of those
kinematic findings.
Because the \girafe{} cohort has a significant sex imbalance (Fisher's
exact $p{=}0.025$) and $f_0$ is strongly sex-dependent,
\cref{tab:gaw} reports results stratified by sex rather than pooled.
The stratified analysis shows that $f_0$ does not distinguish groups
within either sex, confirming the unstratified difference would be
driven by sex composition rather than disease status.
In contrast, cv---the coefficient of variation of the glottal area
waveform---remains the sole feature that survives sex stratification
($p{=}0.006$, female only), capturing the reduced vibratory regularity in
pathological vocal folds due to increased mass and
stiffness~\cite{patel2016effects}.
The automated cv result thus aligns with the variability trends reported
by Patel et al.~\cite{patel2015kinematic}, demonstrating that the
pipeline ``sees'' what clinicians see when distinguishing normal from
disordered voices.
With only \num{12} Healthy and \num{11} Pathological female patients and no
multiple-comparison correction, this result is exploratory and should be
confirmed on a larger, sex-balanced cohort.

\paragraph{Limitations}
The \girafe{} cohort is small (\num{15} Healthy, \num{25} Pathological)
and sex-imbalanced; the male Healthy subgroup ($n{=}3$) is too small for
sex-stratified inference.
With larger, balanced samples the non-significant features may reach
significance.
The \gaw{} analysis uses the \SI{4000}{fps} capture rate of the
high-speed videoendoscope for converting $f_0$ from cycles\slash frame to Hz.
Finally, the \yolocrop{} weights are calibrated to a specific bounding-box
padding (8 px); using a different padding at inference reduces performance.

% ─────────────────────────────────────────────────────────────────────────────
\section{Conclusion}
\label{sec:conclusion}
% ─────────────────────────────────────────────────────────────────────────────

We presented a lightweight U-Net trained with a carefully tuned recipe
(grayscale input, combined BCE\,+\,DSC loss, AdamW with cosine annealing)
that sets a new state of the art on the \girafe{} benchmark
(DSC \num{0.809}, DSC${\geq}0.5 = \SI{96.2}{\%}$), outperforming all
three published baselines and our own detection-gated variants.
We further showed that pairing this U-Net with a YOLOv8 glottis detector
provides a principled robustness mechanism: the detection gate suppresses
spurious predictions on off-target frames, producing clean glottal area
waveforms from full clinical recordings.
A crop-zoom variant (\yolocrop{}) achieves the best performance in a zero-shot
evaluation on the independent \bagls{} dataset (DSC \num{0.659} at
optimised threshold $\tau{=}0.02$), demonstrating that the detection-guided
approach generalises across institutions and equipment.
When U-Net and YOLO are trained on \bagls{}, the pipeline attains DSC \num{0.854} on the full \bagls{} test set (\yolounet{}), surpassing the benchmark baseline~\cite{gomez2020bagls} and diffusion-refined methods~\cite{wu2024medsegdiff}, establishing strong in-distribution results on that benchmark as well.
As a technical validation, we applied the pipeline to all \num{65}
\girafe{} patient recordings and showed that the automatically extracted
coefficient of variation of the glottal area waveform significantly
distinguishes Healthy from Pathological voices even after controlling for
sex imbalance ($p{=}0.006$, female subgroup).
Validation thus goes beyond pixel-level metrics (DSC): the pipeline
replicates established clinical group differences (Healthy vs.\
Pathological) and preserves the coefficient of variation as the key
discriminant for vocal pathology~\cite{patel2015kinematic,patel2016effects}.
By providing a fully automated, detection-gated, zero-shot-capable system,
the pipeline makes these clinically validated kinematic findings
\emph{clinically scalable}---the missing link between the diagnostic value
established in the JSLHR literature and widespread adoption in practice.

% ─────────────────────────────────────────────────────────────────────────────
\section*{Data and Code Availability}

All training and evaluation scripts, trained model weights, and the
\girafe{} evaluation results JSON are available at
\url{https://github.com/hari-krishnan/openglottal}.
The repository README describes dataset splits (training/validation/test) for
both \girafe{} and \bagls{} and explains how to run the detection-gated
pipeline (YOLO detector, U-Net segmenter, and evaluation scripts).
The \girafe{} dataset \cite{andrade2025datainbrief} is freely available from
\texttt{https://zenodo.org/records/13773163}.
The \bagls{} dataset \cite{gomez2020bagls} is available from
\texttt{https://zenodo.org/records/3762320}.

\section*{Author Contributions}
Harikrishnan Unnikrishnan designed the study, implemented the detection-gated
pipeline (YOLO glottis detector, U-Net segmenter, and temporal detector with
4-frame (\SI{1}{ms}) hold for video (\girafe{}); \bagls{} is frame-level so no hold), conducted the experiments on \girafe{} and \bagls{}, performed
the glottal area waveform feature analysis, and wrote the manuscript.

\section*{Declaration of Competing Interest}
The author declares that there are no known competing financial interests or
personal relationships that could have appeared to influence the work reported
in this paper.

\section*{Acknowledgements}
We thank Andrade-Miranda et al.\ for making the \girafe{} dataset publicly
available and G\'{o}mez et al.\ for the \bagls{} benchmark; both datasets were
essential to this work.

% ─────────────────────────────────────────────────────────────────────────────
\bibliographystyle{elsarticle-num}
\bibliography{refs}

\end{document}
